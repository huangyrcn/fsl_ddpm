# =========================
# 基础配置（数据 & Encoder）
# =========================
dataset_name: Letter_high
baseline_mode: null
seed: 72
N_way: 4
K_shot: 3
query_size: 10
patience: 5
dropout: 0.5
batch: 128
gin_layer: 3
gin_hid: 128
aug1: node_drop
aug2: feature_mask
t: 0.2
lr: 0.001
weight_decay: 1.0e-7
eval_interval: 100
epoch_num: 3000
use_select_sim: false
save_test_emb: true

num_token: 1
use_prompt: false
device: cuda
train_classes_num: 13
test_classes_num: 4
node_fea_size: 2
sample_input_size: 256
graph_pooling_type: sum

# Encoder
use_pretrained_encoder: true
encoder_ckpt_path: ./savepoint/Letter_high_encoder.pkl

# =========================
# LDM 训练相关
# =========================
# 是否跳过LDM训练并从ckpt加载（这里为了训练，把它关掉）
use_pretrained_ldm: true
ldm_ckpt_path: ./savepoint/Letter_high_ldm.pkl

# 训练超参
learning_rate_ldm: 1.0e-4
weight_decay_ldm: 1.0e-4
num_epochs_ldm: 30000
patience_ldm: 1000  # 早停容忍（更宽松的早停策略）
# ldm_batch_size: 768    # 显存允许可提
ldm_batch_size: 512 
ldm_es_interval: 200

# 噪声步数（50 足够快；若要更好分布可调 100~250）
# 当前优化目标：细化反向过程，改善FD/MMD，提升协方差匹配度
time_steps: 300
beta_start: 0.0001
beta_end: 0.05

# ===== 新版 LDM 关键参数（新增）=====
ldm_predict: v               # 'v'（推荐）或 'eps'
ldm_unit_sphere: false
ldm_widths: [256, 512, 512]  # Vector-UNet 宽度层级（x_dim=256 更对齐）
ldm_n_blocks: 2
ldm_use_zero_mlp: true

# 训练期的 CFG & 原型正则
ldm_p_uncond: 0.3
ldm_lambda_proto: 0.1

# 训练期条件类型：固定使用 kmeans_proto

# =========================
# 评估/生成（few-shot）相关
# =========================
#ldm性能评估
evaluate_ldm_intrinsic: true
evaluate_ldm_intrinsic_num_samples: 1000

# 生成时的 CFG 引导强度（2~4 常用）
ldm_guidance: 2.5

# 评估专用参数（与增强采样一致）
ldm_guidance_eval: 0.0            # 评估时关闭CFG，避免方差爆炸
ldm_eval_temp: 1.0                # 评估时温度（可选：1.05 如果FD仍顽固）
ldm_eval_simple_var: false        # 评估时使用简单方差

# 测试阶段：每个支持样本生成多少增强
num_augmented_samples: 15

# 生成增强时的采样温度与方差策略
ldm_aug_temp: 0.9          # 建议 0.85~0.95（网格搜索：0.95, 1.0）
ldm_aug_simple_var: true    # 与评估保持一致：用 simple 方差


# 生成期条件类型：固定使用 label_proto

task_finetune_steps: 5000              # 微调步骤数（增加到5000）
task_finetune_lr: 2.0e-4               # 微调学习率
task_finetune_weight_decay: 1.0e-5      # 微调权重衰减，降低衰减强度
task_finetune_cond_dropout: 0.10        # 任务级微调条件dropout（降噪更稳）
task_finetune_grad_clip: 1.0           # 梯度裁剪，防止梯度爆炸
task_finetune_patience: 200            # 增加早停容忍度，防止过早停止
task_finetune_warmup_steps: 200        # 新增：微调LR预热步数
task_lambda_proto: 0.05                # 原型一致性正则（适中）
task_finetune_accum_steps: 4           # 新增：梯度累积步数
task_finetune_micro_repeats: 4         # 新增：每步重复前向次数
task_finetune_ema_alpha: 0.98          # 新增：EMA平滑系数



# =========================
# wandb
# =========================
use_wandb: true
wandb_project: "my_fsl_project"
wandb_run_name: "experiment_001"
wandb_entity: "huangyrcn"
wandb_online: true
