# 基础配置（来自原Letter_high.yaml）
dataset_name: Letter_high
baseline_mode: null
N_way: 4
K_shot: 5
query_size: 10
patience: 50
dropout: 0.5
batch: 512
gin_layer: 3
gin_hid: 128
aug1: node_drop
aug2: feature_mask
t: 0.2
lr: 0.0005
weight_decay: 1.0e-06
eval_interval: 100
epoch_num: 5000
use_select_sim: false
gen_train_num: 0
gen_test_num: 0
save_test_emb: true

num_token: 1
use_prompt: false
device: cuda
train_classes_num: 13
test_classes_num: 4
node_fea_size: 2
sample_input_size: 256

graph_pooling_type: sum

use_pretrained_encoder: true
encoder_ckpt_path: ./savepoint/Letter_high_encoder.pkl
# LDM增强配置
use_ldm_augmentation: true
use_pretrained_ldm: true  # 是否跳过LDM训练并从ckpt加载
ldm_ckpt_path: ./savepoint/Letter_high_ldm.pkl  # 预训练LDM权重（state_dict）

# LDM相关参数  
learning_rate_ldm: 1e-4
weight_decay_ldm: 1.0e-04
num_epochs_ldm: 200000
patience_ldm: 200
time_steps: 500
beta_start: 0.0001
beta_end: 0.02
ldm_batch_size: 512
ldm_ema_decay: 0.9              # LDM训练EMA衰减
ldm_es_interval: 100             # LDM早停检查间隔（与打印间隔一致）

# 条件类型配置
# condition_type: kmeans  # 使用KMeans聚类作为条件
condition_type: self_conditioning  # 使用样本自己作为条件

# 数据增强参数
# 每个支持集样本生成的增强样本数量
# 策略：使用支持集样本作为条件，每个样本生成num_augmented_samples个增强样本
num_augmented_samples: 30

batch_size_for_embedding: 512

# 任务级微调参数
task_finetune_steps: 30
task_finetune_lr: 1e-4
task_finetune_weight_decay: 0.0
task_finetune_cond_dropout: 0.05
task_finetune_snr_cap: 5.0   # 目前无效，留存即可
task_finetune_grad_clip: 1.0
task_finetune_patience: 6




# wandb 配置
use_wandb: true
wandb_project: "my_fsl_project"
wandb_run_name: "experiment_001"
wandb_entity: "huangyrcn"  # 替换为你的团队名称
wandb_online: true