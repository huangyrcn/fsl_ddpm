# 基础配置（来自原Reddit.yaml）
dataset_name: Reddit
baseline_mode: null
N_way: 4
K_shot: 5
query_size: 10
patience: 50
dropout: 0.5
batch: 128
gin_layer: 3
gin_hid: 128
aug1: node_drop
aug2: feature_mask
t: 0.2
lr: 0.001
weight_decay: 1.0e-07
eval_interval: 100
epoch_num: 3000
use_select_sim: false
gen_train_num: 0
gen_test_num: 0
save_test_emb: true

num_token: 1
use_prompt: false
device: cuda
train_classes_num: 7
test_classes_num: 4
node_fea_size: 524
sample_input_size: 256

use_pretrained_encoder: false
encoder_ckpt_path: ./savepoint/Reddit_encoder.pkl
# LDM增强配置
use_ldm_augmentation: true
use_pretrained_ldm: false  # 是否跳过LDM训练并从ckpt加载
ldm_ckpt_path: ./savepoint/Reddit_ldm.pkl  # 预训练LDM权重（state_dict）

# LDM相关参数  
learning_rate_ldm: 0.001
weight_decay_ldm: 1.0e-04
num_epochs_ldm: 200
patience_ldm: 10
time_steps: 100
beta_start: 0.0001
beta_end: 0.02 
ldm_batch_size: 64
ldm_ema_decay: 0.9              # LDM训练EMA衰减
ldm_es_interval: 100             # LDM早停检查间隔（与打印间隔一致）

# 条件类型配置
# condition_type: self_labeling  # 所有阶段都使用样本自己作为条件
condition_type: self_labeling 

# 数据增强参数
# 每个支持集样本生成的增强样本数量
# 策略：使用支持集样本作为条件，每个样本生成num_augmented_samples个增强样本
num_augmented_samples: 5

batch_size_for_embedding: 128

# 任务级微调参数
task_finetune_steps: 0
task_finetune_lr: 5e-6
task_finetune_weight_decay: 0.0
task_finetune_cond_dropout: 0.05
task_finetune_snr_cap: 5.0   # 目前无效，留存即可
task_finetune_grad_clip: 1.0
task_finetune_patience: 6000

# 嵌入修饰参数
refine_alpha: 0.7  # 修饰强度，0.7表示70%原始 + 30%生成

# wandb 配置
use_wandb: true
wandb_project: "my_fsl_project"
wandb_run_name: "Reddit_experiment"
wandb_entity: "huangyrcn"  # 替换为你的团队名称
wandb_online: true

