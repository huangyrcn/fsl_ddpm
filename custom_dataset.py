import torch
import numpy as np
import networkx as nx
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch.utils.data import Dataset
from typing import List, Optional, Tuple
import json
from collections import defaultdict
import pickle as pkl



def load_data(dataset: str, degree_as_tag: bool = True) -> Tuple[List[Data], dict, List]:
    """
    å®Œå…¨æŒ‰ç…§åŸå§‹å®ç°çš„æ–¹å¼åŠ è½½æ•°æ®ï¼Œç¡®ä¿å®Œå…¨ä¸€è‡´
    """
    print(f"Loading custom data: {dataset}")
    
    # R52/COIL ä½¿ç”¨ä¸ dataset.py ä¸€è‡´çš„ pickle æ•°æ®æ ¼å¼ï¼Œè¿™é‡Œç›´æ¥è½¬ä¸º PyG Data è¿”å›
    if dataset in ['R52', 'COIL']:
        print(dataset)
        node_attribures = pkl.load(open(f'datasets/{dataset}/{dataset}_node_attributes.pickle', 'rb'))
        train_set = pkl.load(open(f'datasets/{dataset}/{dataset}_train_set.pickle', 'rb'))
        val_set = pkl.load(open(f'datasets/{dataset}/{dataset}_val_set.pickle', 'rb'))
        test_set = pkl.load(open(f'datasets/{dataset}/{dataset}_test_set.pickle', 'rb'))

        data_list = []
        for sets in [train_set, val_set, test_set]:
            graph2nodes = sets["graph2nodes"]
            graph2edges = sets['graph2edges']
            label2graphs = sets['label2graphs']
            for label, graphs in label2graphs.items():
                for graph_id in graphs:
                    # é‡å»ºèŠ‚ç‚¹æ˜ å°„
                    node_mapping = {}
                    for node in graph2nodes[graph_id]:

                        node_mapping[node] = len(node_mapping)

                    # æ„é€ è¾¹å¹¶è¿›è¡Œæ— å‘è§„èŒƒåŒ–ã€å»é‡ã€å†å¯¹ç§°åŒ–
                    edges = []
                    for u, v in graph2edges[graph_id]:
                        edges.append([node_mapping[u], node_mapping[v]])
                    edge_index = torch.tensor(edges, dtype=torch.long).t() if edges else torch.empty((2, 0), dtype=torch.long)
                    if edge_index.numel() > 0:
                        u, v = edge_index
                        lo = torch.minimum(u, v)
                        hi = torch.maximum(u, v)
                        undirected = torch.stack([lo, hi], dim=0)
                        undirected = torch.unique(undirected.t(), dim=0).t()
                        edge_index = torch.cat([undirected, undirected.flip(0)], dim=1)

                    # èŠ‚ç‚¹ç‰¹å¾
                    x = torch.FloatTensor(node_attribures[graph2nodes[graph_id]])
                    if dataset == 'R52':
                        x = x.unsqueeze(-1)

                    y = torch.tensor(label, dtype=torch.long)

                    data = Data(x=x, edge_index=edge_index, y=y)
                    data.label = int(label)
                    data_list.append(data)

        print(f"# data: {len(data_list)}\n")
        return data_list, None, None
    
    data_list = []
    label_dict = {}
    
    with open(f"./datasets/{dataset}/{dataset}.txt", "r") as f:
        n_g = int(f.readline().strip())
        
        for i in range(n_g):
            # è¯»å–å›¾ä¿¡æ¯
            row = f.readline().strip().split()
            n, graph_label = int(row[0]), int(row[1])
            
            # æ›´æ–°æ ‡ç­¾å­—å…¸ï¼ˆä¸åŸå§‹å®ç°ä¸€è‡´ï¼‰
            if graph_label not in label_dict:
                mapped = len(label_dict)
                label_dict[graph_label] = mapped
            
            # å®Œå…¨æŒ‰ç…§åŸå§‹å®ç°æ„å»ºNetworkXå›¾
            g = nx.Graph()
            node_tags = []
            feat_dict = {}
            
            for j in range(n):
                g.add_node(j)
                row = f.readline().strip().split()
                tmp = int(row[1]) + 2
                if tmp == len(row):
                    row = [int(w) for w in row]
                else:
                    row = [int(w) for w in row[:tmp]]
                
                # èŠ‚ç‚¹æ ‡ç­¾å¤„ç†ï¼ˆä¸åŸå§‹å®ç°ä¸€è‡´ï¼‰
                if not row[0] in feat_dict:
                    mapped = len(feat_dict)
                    feat_dict[row[0]] = mapped
                node_tags.append(feat_dict[row[0]])
                
                # æŒ‰åŸå§‹é¡ºåºæ·»åŠ è¾¹
                for k in range(2, len(row)):
                    g.add_edge(j, row[k])
            
            # å¦‚æœä½¿ç”¨åº¦ä½œä¸ºæ ‡ç­¾ï¼Œè¦†ç›–èŠ‚ç‚¹æ ‡ç­¾ï¼ˆä¸åŸå§‹å®ç°ä¸€è‡´ï¼‰
            if degree_as_tag:
                node_tags = list(dict(g.degree).values())
            
            # æ„å»ºPyGæ ¼å¼çš„è¾¹ç´¢å¼•
            edges = [list(pair) for pair in g.edges()]
            edges.extend([[i, j] for j, i in edges])
            if edges:
                edge_index = torch.tensor(edges, dtype=torch.long).t()
            else:
                edge_index = torch.empty((2, 0), dtype=torch.long)
            
            # å­˜å‚¨æ•°æ®
            raw_data = {
                'edge_index': edge_index,
                'node_tags': node_tags,
                'graph_label': graph_label,
                'n_nodes': n
            }
            data_list.append(raw_data)
    
    # æ”¶é›†æ‰€æœ‰æ ‡ç­¾æ„å»ºtagsetï¼ˆä¸åŸå§‹å®ç°ä¸€è‡´ï¼‰
    global_tag_set = set()
    for raw_data in data_list:
        global_tag_set.update(raw_data['node_tags'])
    
    tagset = sorted(global_tag_set)
    tag2index = {tag: i for i, tag in enumerate(tagset)}
    
    # æœ€ç»ˆè½¬æ¢ä¸ºPyG Dataå¯¹è±¡
    final_data_list = []
    for raw_data in data_list:
        # æ„å»ºone-hotç‰¹å¾
        x = torch.zeros(raw_data['n_nodes'], len(tagset))
        for i, tag in enumerate(raw_data['node_tags']):
            x[i, tag2index[tag]] = 1
        
        # åˆ›å»ºæœ€ç»ˆçš„Dataå¯¹è±¡
        data = Data(
            x=x,
            edge_index=raw_data['edge_index'],
            y=torch.tensor(raw_data['graph_label'], dtype=torch.long)
        )
        data.label = raw_data['graph_label']
        final_data_list.append(data)
    
    print(f"# classes: {len(label_dict)}")
    print(f"# maximum node tag: {len(tagset)}")
    print(f"# data: {len(final_data_list)}\n")
    
    return final_data_list, label_dict, tagset
    

class CustomDataset:
    """
    å®Œå…¨åŸºäºPyG Dataçš„æ•°æ®é›†ç±»ï¼š
    - ä»åŠ è½½å¼€å§‹å°±ä½¿ç”¨PyG Dataæ ¼å¼
    - åœ¨initä¸­é¢„å…ˆåˆ’åˆ†æ”¯æŒé›†å’ŒæŸ¥è¯¢é›†æ± 
    - sample_one_taskåªéœ€è¦ä»»åŠ¡IDå³å¯è·å–æ•°æ®
    """

    def __init__(self, name, args):
        self.dataset_name = name
        self.args = args
        self.train_graphs = []
        self.test_graphs = []

        # ç›´æ¥åŠ è½½ä¸ºPyG Dataå¯¹è±¡
        all_graphs, label_dict, tagset = load_data(self.dataset_name, True)
        self.tagset = tagset
        self.label_dict = label_dict
        # è®¾ç½®ç‰¹å¾ç»´åº¦ï¼ˆèŠ‚ç‚¹ç‰¹å¾çš„ç»´åº¦ï¼‰ï¼Œå…¼å®¹ R52/COILï¼ˆtagset ä¸º Noneï¼‰
        self.feature_dim = (len(tagset) if tagset is not None else (all_graphs[0].x.size(1) if all_graphs else 0))

        # åŠ è½½ç±»åˆ«åˆ’åˆ†
        with open("datasets/{}/train_test_classes.json".format(args.dataset_name), "r") as f:
            all_class_splits = json.load(f)

        # è·å–è®­ç»ƒå’Œæµ‹è¯•ç±»åˆ«
        train_classes = all_class_splits['train']
        test_classes = all_class_splits['test']

        # å›¾æŒ‰ç±»åˆ«åˆ’åˆ†ä¸ºè®­ç»ƒå›¾å’Œæµ‹è¯•å›¾ï¼ˆå·²ç»æ˜¯PyG Dataæ ¼å¼ï¼‰
        # åŒæ—¶é‡æ–°æ˜ å°„æ ‡ç­¾ï¼Œä¸dataset.pyä¿æŒä¸€è‡´
        train_classes_mapping = {}
        for cl in train_classes:
            train_classes_mapping[cl] = len(train_classes_mapping)
        
        test_classes_mapping = {}
        for cl in test_classes:
            test_classes_mapping[cl] = len(test_classes_mapping)
            
        for data in all_graphs:
            if data.label in train_classes:
                # é‡æ–°æ˜ å°„è®­ç»ƒé›†æ ‡ç­¾
                data.label = train_classes_mapping[data.label]
                data.y = torch.tensor(data.label, dtype=torch.long)  # åŒæ­¥æ›´æ–°y
                self.train_graphs.append(data)
            elif data.label in test_classes:
                # é‡æ–°æ˜ å°„æµ‹è¯•é›†æ ‡ç­¾  
                data.label = test_classes_mapping[data.label]
                data.y = torch.tensor(data.label, dtype=torch.long)  # åŒæ­¥æ›´æ–°y
                self.test_graphs.append(data)

        print(f"# train graphs: {len(self.train_graphs)}, # test graphs: {len(self.test_graphs)}")

        # æ·»åŠ å…¼å®¹æ€§å±æ€§
        self.train_classes_num = len(train_classes_mapping)
        self.test_classes_num = len(test_classes_mapping)
        self.train_classes = train_classes
        self.test_classes = test_classes
        
        # === æ·»åŠ ä¸dataset.pyä¸€è‡´çš„shuffleé€»è¾‘ ===
        np.random.seed(args.seed)
        np.random.shuffle(self.train_graphs)
        
        np.random.seed(args.seed)
        np.random.shuffle(self.test_graphs)

        # æ„å»ºtaskåˆ—è¡¨ï¼ˆå…¼å®¹æ€§æ¥å£ï¼‰
        self.train_tasks = defaultdict(list)
        for data in self.train_graphs:
            self.train_tasks[data.label].append(data)
            
        # æµ‹è¯•ç±»åˆ«æ„å»º
        self.test_tasks = defaultdict(list)
        for data in self.test_graphs:
            self.test_tasks[data.label].append(data)

        # === æ„å»ºnwayçš„å…¨å±€æŸ¥è¯¢é›†æ±  ===
        self.test_fine_tune_list = []
        self.total_test_g_list = []
        for index in range(self.args.N_way):
            # æ¯ä¸ªç±»åˆ«çš„å‰K_shotä¸ªä½œä¸ºæ”¯æŒé›†å€™é€‰
            self.test_fine_tune_list.append(list(self.test_tasks[index])[:args.K_shot])
            # å‰©ä½™çš„åŠ å…¥å…¨å±€æŸ¥è¯¢é›†æ± 
            self.total_test_g_list.extend(list(self.test_tasks[index])[args.K_shot:])
        
        # nwayçš„ shuffle
        np.random.seed(args.seed)
        np.random.shuffle(self.total_test_g_list)
        self.test_task_Num=len(self.total_test_g_list) // (self.args.N_way * self.args.query_size)

    def aug(self,
            aug1_type: str,
            aug2_type: str,
            batch_size: int = 32,
            shuffle: bool = True,
            num_workers: int = 0,
            pin_memory: bool = False,
            seed: int = None,
            aug_ratio: float = 0.1):
        """
        è¿”å›ä¸¤ä¸ªå¯¹é½çš„ DataLoaderï¼Œç”¨äºå¯¹æ¯”å­¦ä¹ çš„ 1:1 å¢å¼ºè§†å›¾ã€‚
        - è§†å›¾1ä½¿ç”¨ aug1_type
        - è§†å›¾2ä½¿ç”¨ aug2_type
        ä¸¤ä¸ª DataLoader ä½¿ç”¨ç›¸åŒçš„æ‰“ä¹±é¡ºåºï¼Œä¿è¯æ ·æœ¬ä¸€ä¸€å¯¹åº”ã€‚
        """
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
        from aug import get_optimized_augmentation

        # åœ¨ CPU ä¸Šè¿›è¡Œå¢å¼ºï¼Œé¿å…ä¸åŸå§‹ Data çš„è®¾å¤‡ä¸ä¸€è‡´
        aug1 = get_optimized_augmentation(aug_type=aug1_type, aug_ratio=aug_ratio, device='cpu', seed=(seed if seed is not None else self.args.seed))
        aug2 = get_optimized_augmentation(aug_type=aug2_type, aug_ratio=aug_ratio, device='cpu', seed=(seed if seed is not None else self.args.seed))

        # å…ˆå¯¹ train_graphs è¿›è¡Œä¸€æ¬¡å…¨å±€æ‰“ä¹±ï¼ŒéšååŸºäºè¯¥å›ºå®šé¡ºåºåšä¸¤ç§å¢å¼º
        if shuffle:
            s = int(seed if seed is not None else self.args.seed)
            gperm = torch.Generator(); gperm.manual_seed(s)
            perm = torch.randperm(len(self.train_graphs), generator=gperm).tolist()
        else:
            perm = list(range(len(self.train_graphs)))

        # åŸºäº list çš„å¢å¼ºï¼šå…ˆç”Ÿæˆä¸¤ä¸ªå¯¹é½çš„å¢å¼ºåˆ—è¡¨ï¼Œå†æ„å»º DataLoader
        aug_list1 = []
        aug_list2 = []
        for i in perm:
            d = self.train_graphs[i]
            aug_list1.append(aug1(d))
            aug_list2.append(aug2(d))

        # DataLoader ä¸å†é¢å¤– shuffleï¼Œä¿éšœä¸¤ä¸ªè§†å›¾ä¸¥æ ¼å¯¹é½
        loader_kwargs = dict(batch_size=batch_size,
                              shuffle=False,
                              num_workers=num_workers,
                              pin_memory=pin_memory)

        loader1 = DataLoader(aug_list1, **loader_kwargs)
        loader2 = DataLoader(aug_list2, **loader_kwargs)

        return loader1, loader2

    def sample_one_task(self, task_id):
        # æ„å»ºæ”¯æŒé›†ï¼šä½¿ç”¨sumå±•å¹³
        support_graphs = sum(self.test_fine_tune_list, [])
        
        # ä»å…¨å±€æŸ¥è¯¢é›†æ± ä¸­æŒ‰task_id é¡ºåºå–æ ·æœ¬
        total_query_needed = self.args.N_way * self.args.query_size
        query_start_idx = task_id * total_query_needed
        
        query_graphs = []
        for i in range(total_query_needed):
            query_idx = (query_start_idx + i) % len(self.total_test_g_list)
            query_graphs.append(self.total_test_g_list[query_idx])
        
        return support_graphs, query_graphs

    def get_encoder_trainloader(self, batch_size: int = 64, shuffle: bool = True, num_workers: int = 0, pin_memory: bool = False, seed: int = None) -> DataLoader:
        """è·å–ç¼–ç å™¨è®­ç»ƒçš„PyG DataLoader"""
        # ğŸ”§ ä¿®å¤å¯é‡ç°æ€§ï¼šä¸º DataLoader è®¾ç½®å›ºå®šçš„éšæœºæ•°ç”Ÿæˆå™¨
        generator = None
        if shuffle and seed is not None:
            generator = torch.Generator()
            generator.manual_seed(seed)
        
        return DataLoader(
            dataset=self.train_graphs,
            batch_size=batch_size, 
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=pin_memory,
            generator=generator  # ğŸ¯ å…³é”®ä¿®å¤ï¼šå›ºå®š shuffle çš„éšæœºæ€§
        )

    def get_ldm_inference_batches(self, batch_size: int = 64, num_workers: int = 0) -> DataLoader:
        """è·å–LDMæ¨ç†ç”¨çš„PyG DataLoader"""
        return DataLoader(
            dataset=self.train_graphs,
            batch_size=batch_size,
            shuffle=False,  # æ¨ç†ä¸éœ€è¦shuffle
            num_workers=num_workers,
            pin_memory=False
        )


if __name__ == '__main__':
    # æœ€å°è‡ªæ£€ï¼šåŠ è½½ R52 ä¸ COILï¼Œæ‰“å°è®­ç»ƒ/æµ‹è¯•å›¾æ•°é‡ä¸ç±»åˆ«æ•°
    class _Args:
        def __init__(self, dataset_name: str):
            self.dataset_name = dataset_name
            self.seed = 42
            self.N_way = 3
            self.K_shot = 5
            self.query_size = 10

    for ds in ['ENZYMES', 'Letter_high', 'TRIANGLES','Reddit', 'R52', 'COIL']:
        try:
            print(f"\n=== Sanity Check: {ds} ===")
            args = _Args(ds)
            dset = CustomDataset(ds, args)
            print(f"train_graphs: {len(dset.train_graphs)} | test_graphs: {len(dset.test_graphs)}")
            print(f"train_classes_num: {dset.train_classes_num} | test_classes_num: {dset.test_classes_num}")
            # é‡‡æ ·ä¸€ä¸ªæµ‹è¯•ä»»åŠ¡
            support, query = dset.sample_one_task(task_id=0)
            print(f"sampled support: {len(support)} | query: {len(query)}")
        except Exception as e:
            print(f"[ERROR] {ds}: {e}")